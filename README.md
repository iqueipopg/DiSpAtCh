![DiSpAtCh Banner](images/Banner.png)

# DiSpAtCh ğŸ¤–ğŸ“¦

**DiSpAtCh** (*Deep Intelligent Spatial Agent for Task CHaining*) is a Deep Reinforcement Learning system that trains an autonomous agent to navigate a warehouse environment, pick up objects from shelves, and deliver them to a designated area.

This project was developed as part of the **Reinforcement Learning** course at **Universidad Pontificia Comillas, ICAI**, within the **Engineering Mathematics (iMAT)** program.

> ğŸ¯ *From random wandering to efficient delivery â€“ DiSpAtCh learns warehouse logistics through trial and error.*

---

## ğŸ“œ Table of Contents
- [ğŸ“Œ Project Overview](#-project-overview)
- [ğŸ® Environments](#-environments)
- [ğŸ› ï¸ Installation](#ï¸-installation)
- [ğŸš€ How to Use](#-how-to-use)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ“Š Results](#-results)
- [ğŸ§  Technologies Used](#-technologies-used)
- [ğŸ™Œ Credits](#-credits)

---

## ğŸ“Œ Project Overview

DiSpAtCh implements a **Double DQN (Deep Q-Network)** agent that learns to operate in a simulated warehouse environment. The agent must learn to:

1. **Navigate** around obstacles (shelves) without collisions
2. **Pick up** objects from fixed or random positions
3. **Deliver** objects to a designated green area

### ğŸ§  Key Features

- **Double DQN** with experience replay for stable learning
- **Prioritized Experience Replay** boosting positive experiences
- **Rich feature representation** (23 features) with obstacle proximity detection
- **Transfer Learning** between environments for faster convergence
- **Best model checkpointing** based on success rate during training
- **Early stopping** when target success rate is achieved

### ğŸ­ The Warehouse

<p align="center">
  <img src="images/env.png" width="80%">
  <br><em>The Warehouse</em>
</p>

---

## ğŸ® Environments

The project includes three environments of increasing difficulty:

| Environment | Objects | Objective | Actions | Difficulty |
|-------------|---------|-----------|---------|------------|
| **Entorno 1** | Fixed positions | Pick only | 5 (â†‘â†“â†â†’ pick) | â­ |
| **Entorno 2** | Fixed positions | Pick + Delivery | 6 (â†‘â†“â†â†’ pick drop) | â­â­ |
| **Entorno 3** | Random positions | Pick + Delivery | 6 (â†‘â†“â†â†’ pick drop) | â­â­â­ |

### Reward Structure

| Event | Reward |
|-------|--------|
| Each step | -1 |
| Collision (wall/shelf) | -100 |
| Successful pick | +100 |
| Successful delivery | +200 |
| Drop outside area | -50 |
| Invalid action | -1 |

### Training Strategy

```
Entorno 1 (from scratch)
    â†“ Transfer Learning
Entorno 2 (5â†’6 actions, partial transfer)
    â†“ Transfer Learning  
Entorno 3 (full transfer, learns generalization)
```

---

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- PyTorch (with CUDA support recommended)
- Gymnasium

### Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/DiSpAtCh.git
   cd DiSpAtCh
   ```

2. Create a virtual environment (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate      # Linux/macOS
   .\venv\Scripts\activate       # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸš€ How to Use

### Training

Train agents on each environment sequentially:

```bash
cd src

# Train Environment 1 (required first)
python entrenar_entorno1.py

# Train Environment 2 (uses transfer learning from E1)
python entrenar_entorno2.py

# Train Environment 3 (uses transfer learning from E2)
python entrenar_entorno3.py
```

Or train all environments in sequence:

```bash
python entrenar_todos.py
```

The best model (by success rate) is automatically saved to `models/`.

### Evaluating Models

Run comprehensive evaluation with visualizations:

```bash
python evaluar_entornos.py
```

This generates multiple analysis graphs in `outputs/`.

### Visualizing an Agent

Watch a trained agent in action:

```bash
# Visualize Environment 1 (3 episodes by default)
python visualizar_agente.py 1

# Visualize Environment 2 with 5 episodes
python visualizar_agente.py 2 5

# Visualize Environment 3 with 10 episodes
python visualizar_agente.py 3 10
```

---

## ğŸ“‚ Project Structure

```
DiSpAtCh/
â”‚
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ almacen_env.py          # Warehouse environment (Gymnasium)
â”‚   â”œâ”€â”€ representacion.py        # Feature extraction (23 features)
â”‚   â”œâ”€â”€ agente_dqn.py           # DQN agent implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ entrenar_entorno1.py    # Training script for Environment 1
â”‚   â”œâ”€â”€ entrenar_entorno2.py    # Training script for Environment 2
â”‚   â”œâ”€â”€ entrenar_entorno3.py    # Training script for Environment 3
â”‚   â”œâ”€â”€ entrenar_todos.py       # Train all environments sequentially
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluar_entornos.py     # Evaluation with graphs
â”‚   â””â”€â”€ visualizar_agente.py    # Real-time agent visualization
â”‚
â”œâ”€â”€ models/                      # Trained models
â”‚   â”œâ”€â”€ entorno1_mejor.pth      # Best model for Environment 1
â”‚   â”œâ”€â”€ entorno2_mejor.pth      # Best model for Environment 2
â”‚   â””â”€â”€ entorno3_mejor.pth      # Best model for Environment 3
â”‚
â””â”€â”€ outputs/                     # Training graphs and results
    â”œâ”€â”€ entorno1_training.png
    â”œâ”€â”€ entorno2_training.png
    â”œâ”€â”€ entorno3_training.png
    â”œâ”€â”€ grafico_comparacion_objetivos.png
    â”œâ”€â”€ grafico_histograma_pasos.png
    â”œâ”€â”€ grafico_resultados_pie.png
    â”œâ”€â”€ grafico_success_acumulado.png
    â””â”€â”€ grafico_tabla_resumen.png
```

---

## ğŸ“Š Results

### Performance Summary

| Environment | Target | Success Rate | Collision Rate | Avg Reward |
|-------------|--------|--------------|----------------|------------|
| **Entorno 1** | â‰¥95% | **97.4%** âœ… | 1.2% | 82.5 |
| **Entorno 2** | â‰¥90% | **91.4%** âœ… | 3.1% | 228.3 |
| **Entorno 3** | â‰¥85% | **88.4%** âœ… | 4.2% | 215.7 |

### Training Progress

#### Environment 1: Pick Only
- **Episodes to converge:** ~700
- **Training time:** ~2.5 minutes
- **Key insight:** Rapid learning with epsilon decay

#### Environment 2: Pick + Delivery
- **Episodes to converge:** ~8000
- **Training time:** ~25 minutes
- **Key insight:** Discovers DROP action around episode 3000, then rapid improvement

#### Environment 3: Random Objects (Generalization)
- **Episodes to converge:** ~2100
- **Training time:** ~8 minutes
- **Key insight:** Transfer learning enables fast adaptation to random positions

### Training Curves

<p align="center">
  <img src="outputs/entorno1_training.png" width="80%">
  <br><em>Environment 1: Rapid convergence to optimal policy</em>
</p>

<p align="center">
  <img src="outputs/entorno2_training.png" width="80%">
  <br><em>Environment 2: Plateau until DROP action is discovered</em>
</p>

<p align="center">
  <img src="outputs/entorno3_training.png" width="80%">
  <br><em>Environment 3: Fast adaptation thanks to transfer learning</em>
</p>

---

## ğŸ§  Technologies Used

### Frameworks & Libraries

- **PyTorch** â€“ Deep learning framework for DQN implementation
- **Gymnasium** â€“ RL environment interface
- **NumPy** â€“ Numerical computations
- **Matplotlib** â€“ Training visualization and environment rendering

### RL Techniques

| Technique | Purpose |
|-----------|---------|
| **Double DQN** | Reduces overestimation bias in Q-learning |
| **Experience Replay** | Breaks correlation between consecutive samples |
| **Prioritized Replay** | Boosts learning from successful experiences |
| **Target Network** | Stabilizes training with periodic weight updates |
| **Epsilon-Greedy** | Balances exploration vs exploitation |
| **Transfer Learning** | Accelerates training on harder environments |
| **Early Stopping** | Prevents overfitting when target is reached |

### Neural Network Architecture

```
Input (23 features)
    â†“
Linear(23 â†’ 128) + ReLU
    â†“
Linear(128 â†’ 64) + ReLU
    â†“
Linear(64 â†’ num_actions)
    â†“
Output (Q-values)
```

### Feature Engineering (23 features)

| Features | Count | Description |
|----------|-------|-------------|
| Agent position | 2 | Normalized (x, y) |
| Has object flag | 1 | Binary |
| Distance to objects | 3 | Normalized distances |
| Closest object distance | 1 | Minimum distance |
| Direction to closest | 2 | Unit vector |
| Distance to delivery | 1 | Normalized |
| Direction to delivery | 2 | Unit vector |
| Relative positions | 6 | Objects relative to agent |
| **Obstacle proximity** | 4 | Distance to obstacles in 4 directions |
| Can pick flag | 1 | Binary (close enough to pick) |

---

## ğŸ™Œ Credits

This project was developed as part of the **Reinforcement Learning** course at **Universidad Pontificia Comillas, ICAI**.

### Team Members

- **BeltrÃ¡n SÃ¡nchez Careaga**
- **Jorge Kindelan Navarro**
- **Ignacio Queipo de Llano PÃ©rez-GascÃ³n**

### Acknowledgments

- Our professors for their guidance throughout the course
- The **PyTorch** and **Gymnasium** communities for excellent documentation

---

## ğŸ“„ License

This project is for educational purposes as part of the iMAT program at ICAI.
